from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
import random

def fine_tune_embeddings(train_data, model_name="multi-qa-mpnet-base-dot-v1", output_path="fine_tuned_model"):
    # Load pretrained model
    model = SentenceTransformer(model_name)

    # Prepare and shuffle training examples
    train_examples = [InputExample(texts=[query, relevant_context]) for query, relevant_context in train_data]
    random.shuffle(train_examples)

    # DataLoader with smart batching
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8, collate_fn=model.smart_batching_collate)

    # Define loss
    train_loss = losses.MultipleNegativesRankingLoss(model)

    # Fine-tune the model with adjusted parameters
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=2,                # Number of epochs
        warmup_steps=50,         # Warmup steps for stable training
        optimizer_params={'lr': 1e-5},  # Lower learning rate for fine-tuning
        show_progress_bar=True   # Show progress for monitoring
    )

    # Save fine-tuned model
    model.save(output_path)
    print(f"Fine-tuned model saved at {output_path}")

# Example usage
if __name__ == "__main__":
    train_data = [
        ("What is the primary goal of Natural Language Processing (NLP)?", 
         "To develop computational models for understanding and generating natural language."),
        ("What does RAG stand for in NLP?", "RAG stands for Retrieval-Augmented Generation."),
        ("Name two applications of Conditional Random Fields (CRFs) discussed in the course.", 
         "Named Entity Recognition (NER) and Part-of-Speech (POS) tagging."),
        ("What is perplexity in language modeling?", 
         "Perplexity measures how well a language model predicts a sample and is derived from cross-entropy."),
        ("How do Recurrent Neural Networks (RNNs) handle sequential data?", 
         "By maintaining a hidden state that captures information about previous time steps."),
        ("What distinguishes Linear Chain CRFs from Hidden Markov Models (HMMs)?", 
         "CRFs are discriminative models focusing on the conditional probability of labels given input, while HMMs are generative."),
        ("What is the purpose of smoothing in language models?", 
         "To handle unseen words or sequences by adjusting probability estimates."),
        ("Define the Viterbi algorithm in HMMs.", 
         "It finds the most likely sequence of hidden states for a given sequence of observations."),
        ("What is Zipf's Law in linguistics?", 
         "It states that the frequency of words in a corpus is inversely proportional to their rank."),
        ("What are some challenges of Part-of-Speech (POS) tagging?", 
         "Ambiguity in word meanings and context-dependent usages."),
        ("How are neural networks different from logistic regression in classification?", 
         "Neural networks can model complex non-linear functions, unlike logistic regression, which is linear."),
        ("What is cross-validation in machine learning?", 
         "A technique where the dataset is split into folds for training and validation to ensure robustness."),
        ("What are open and closed word classes?", 
         "Open classes (e.g., nouns, verbs) accept new words, while closed classes (e.g., pronouns, prepositions) are relatively fixed."),
        ("What are common evaluation metrics for classifiers in NLP?", 
         "Accuracy, precision, recall, and F1-score."),
        ("Describe the 'forward algorithm' in HMMs.", 
         "It computes the probability of an observation sequence using dynamic programming."),
        ("How does an N-gram model represent context?", 
         "It uses the previous N-1 words to predict the next word."),
        ("What is the softmax layer used for in neural networks?", 
         "To normalize outputs into probabilities for classification."),
        ("How does Baum-Welch training optimize HMMs?", 
         "By iteratively estimating expected counts and maximizing the likelihood."),
        ("What is a unigram model in NLP?", 
         "A language model assuming each word is independent of others."),
        ("Why are language models evaluated with perplexity?", 
         "It gives a more interpretable measure of predictive performance."),
        ("What are the steps in building a text classifier?", 
         "Define the problem, extract features, train the model, and evaluate on test data."),
        ("Name an example of a sequence labeling task.", 
         "Part-of-Speech tagging."),
        ("What is the difference between generative and discriminative models?", 
         "Generative models model joint probabilities, while discriminative models model conditional probabilities."),
        ("What is a 'hidden state' in RNNs?", 
         "A representation of information accumulated over previous time steps."),
        ("Why is feature extraction important in classification tasks?", 
         "It transforms raw data into a form suitable for machine learning models."),
        ("What is the role of smoothing in maximum likelihood estimation?", 
         "It prevents zero probabilities by redistributing probabilities across events."),
        ("How does a CRF handle sequence modeling?", 
         "By modeling dependencies between labels in a sequence."),
        ("What are the components of HMM parameters?", 
         "Initial probabilities, transition probabilities, and emission probabilities."),
        ("What is the sigmoid function in neural networks?", 
         "An activation function mapping values to a range of [0, 1]."),
        ("What are some uses of supervised learning in NLP?", 
         "Spam detection, sentiment analysis, and POS tagging."),
        ("What does CYK stand for in parsing algorithms?", 
         "Cocke-Younger-Kasami, a dynamic programming algorithm for CFG parsing."),
        ("What are the main steps of the CYK algorithm?", 
         "Convert CFG to CNF, set up a table, fill in the table, and recover parses."),
        ("What is Chomsky Normal Form (CNF)?", 
         "A grammar where rules are of the form A → BC or A → a."),
        ("What is the principle of compositionality?", 
         "The meaning of a phrase depends on the meanings of its parts."),
        ("What are the components of a PCFG?", 
         "Non-terminals, terminals, a start symbol, and production rules with probabilities."),
        ("What is the difference between CFG and dependency grammar?", 
         "CFG focuses on constituents while dependency grammar represents head-dependent relations."),
        ("What does 'semantic inference' involve?", 
         "Making explicit something that is implicit in language."),
        ("What is an example of lexical semantic relations?", 
         "Hypernymy, synonymy, antonymy, and polysemy."),
        ("What is Lesk's algorithm used for?", 
         "Disambiguating word senses using dictionary definitions."),
        ("What is Yarowsky's algorithm based on?", 
         "Bootstrapping with minimal supervision for word sense disambiguation."),
        ("What is pointwise mutual information (PMI)?", 
         "A measure of how much two words are correlated above chance."),
        ("What are Hearst patterns used for?", 
         "Identifying hyponym-hypernym relationships in text."),
        ("What is a term-context matrix?", 
         "A matrix where rows represent target words and columns represent context words."),
        ("What does SVD stand for in distributional semantics?", 
         "Singular Value Decomposition, used to reduce dimensions in term-context matrices."),
        ("What is the purpose of word embeddings?", 
         "To represent words as vectors in a continuous space for capturing semantic similarities."),
        ("What is the difference between CBOW and Skip-gram models in word2vec?", 
         "CBOW predicts a target word from context, while Skip-gram predicts context from a target word."),
        ("What is dependency grammar?", 
         "A grammar that represents the syntactic structure through head-dependent relations."),
        ("What is Montagovian semantics?", 
         "A formalism using logical expressions to represent the meaning of sentences."),
        ("What is the difference between homonymy and polysemy?", 
         "Homonymy refers to unrelated meanings of a word, while polysemy refers to related meanings."),
        ("What does 'semantic attachment' refer to in compositional semantics?", 
         "Augmenting syntactic rules with lambda expressions for meaning computation."),
        ("What is the primary challenge in machine translation (MT)?", 
         "The lack of one-to-one lexical mapping, syntactic differences, and cultural variations between languages."),
        ("What is the Sapir-Whorf hypothesis?", 
         "The idea that language influences thought, with a strong version suggesting it constrains all thoughts."),
        ("What is the role of the noisy channel model in MT?", 
         "It models translation as finding the target language sentence with the highest probability given the source sentence."),
        ("What is the purpose of BLEU in MT evaluation?", 
         "To measure the overlap between machine-generated translations and reference translations using n-grams."),
        ("What is phrase-based SMT?", 
         "A statistical MT approach that translates and reorders phrases instead of individual words."),
        ("What are the key steps of the CYK algorithm?", 
         "Converting a grammar to CNF, building a parse table, and retrieving parse trees."),
        ("What are microplanning and surface realization in NLG?", 
         "Microplanning determines sentence structures, while surface realization converts plans into actual text."),
        ("What is abstraction in summarization?", 
         "Synthesizing new text not directly found in the source to better convey information."),
        ("What is the difference between extraction and abstraction in summarization?", 
         "Extraction copies content directly, while abstraction generates new content."),
        ("What is coreference resolution?", 
         "The task of identifying all expressions in a text that refer to the same entity."),
        ("What are discourse markers?", 
         "Linguistic cues like 'however' or 'therefore' that indicate relationships between parts of a text."),
        ("What is zero anaphora?", 
         "The omission of pronouns or other referents in a sentence, common in pro-drop languages."),
        ("What are the three steps of extractive summarization?", 
         "Analysis to select content, transformation to refine it, and synthesis to organize it."),
        ("What are key challenges in cross-lingual transfer learning?", 
         "Limited labeled data, different scripts, and low resource language coverage."),
        ("What is language adaptation fine-tuning (LAFT)?", 
         "A method to adapt pre-trained language models to new languages using monolingual corpora."),
        ("What is the attention mechanism in NMT?", 
         "A method to focus on relevant parts of the source sentence when generating translations."),
        ("What is a transformer in NLP?", 
         "An architecture using self-attention to process sequences in parallel, replacing recurrent models."),
        ("What are keys, queries, and values in transformers?", 
         "They are vectors used to compute attention weights and focus on relevant information."),
        ("What is the role of the encoder and decoder in NMT?", 
         "The encoder converts input into a representation, and the decoder generates the target sequence."),
        ("What is integer linear programming in NLG?", 
         "A method to optimize sentence fusion by imposing grammatical and semantic constraints."),
        ("What are the key steps in building a text classifier?",
        "Define the problem, extract features, train the classifier, and evaluate on test data."),
        ("What is perplexity in the context of language models?",
        "A measure of how well a language model predicts a sequence, derived from cross-entropy."),
        ("What is the difference between unigrams, bigrams, and trigrams?",
        "Unigrams consider one word, bigrams two consecutive words, and trigrams three consecutive words in a sequence."),
        ("Why is smoothing necessary in language models?",
        "To adjust probability distributions and handle unseen words or n-grams during testing."),
        ("What is the role of a feature vector in classification?",
        "It represents a document's properties that are relevant for predicting its category."),
        ("What is Zipf’s Law in linguistics?",
        "It states that word frequency is inversely proportional to its rank in a corpus."),
        ("How does Naïve Bayes classify data?",
        "By applying Bayes’ rule, assuming feature independence, and estimating probabilities from training data."),
        ("What is the purpose of a loss function in neural networks?",
        "To quantify the error between predicted and actual outputs, guiding model optimization."),
        ("What are activation functions, and why are they important in neural networks?",
        "Functions like ReLU and sigmoid introduce non-linearity, enabling networks to model complex patterns."),
        ("What is cross-entropy loss used for in classification tasks?",
        "To measure the dissimilarity between predicted probabilities and true labels in a multi-class setup."),
        ("What is the chain rule in probability, and how is it used in n-gram models?",
        "It decomposes joint probabilities into conditional probabilities, forming the basis for n-gram modeling."),
        ("What is the difference between discriminative and generative classifiers?",
        "Discriminative models predict outputs directly from inputs, while generative models learn joint distributions."),
        ("What are some applications of linear classifiers in NLP?",
        "Sentiment analysis, spam detection, and authorship attribution."),
        ("What is word segmentation, and why is it challenging for some languages?",
        "The process of dividing text into words, which is difficult in languages like Chinese that lack spaces."),
        ("What is overfitting, and how does it impact language models?",
        "When a model fits training data too closely, reducing its generalization to unseen data."),
        ("What is the role of the softmax layer in classification tasks?",
        "To convert raw model outputs into probabilities for discrete outcomes."),
        ("How does supervised learning differ from unsupervised learning in NLP?",
        "Supervised learning uses labeled data to map inputs to outputs, while unsupervised learning finds patterns without labels."),
        ("What is stemming in NLP?",
        "A process that reduces words to their root form by stripping affixes, often using heuristic rules."),
        ("What are discourse relations in text classification?",
        "Relationships like explanation, cause, and elaboration between clauses or sentences."),
        ("How is word frequency used in NLP tasks?",
        "It aids in feature extraction, text classification, and understanding corpus structure."),
        ("What are the components of a Hidden Markov Model (HMM)?",
        "Initial probabilities, transition probabilities, and emission probabilities."),
        ("What is the purpose of the forward algorithm in HMMs?",
        "To compute the likelihood of an observation sequence efficiently."),
        ("What is the role of the Viterbi algorithm in sequence labeling?",
        "To find the most likely sequence of hidden states given the observations."),
        ("What is Chomsky Normal Form (CNF) in parsing?",
        "A grammar form where each production is either A → BC or A → a, simplifying parsing algorithms."),
        ("Why is the CKY algorithm important in NLP?",
        "It uses dynamic programming to efficiently find all possible parse trees for a sentence."),
        ("What are open word classes in linguistics?",
        "Word classes like nouns and verbs that frequently accept new words."),
        ("What is a key limitation of HMMs in NLP tasks?",
        "They assume independence between observations, limiting their ability to capture complex dependencies."),
        ("How does a Linear-Chain Conditional Random Field (LC-CRF) improve over HMMs?",
        "LC-CRFs allow for incorporating overlapping and rich features into sequence labeling."),
        ("What are some uses of dependency grammar?",
        "To represent grammatical relations as directed edges between words, aiding in syntactic analysis."),
        ("What are the steps involved in the Baum-Welch algorithm?",
        "Estimate expected counts using forward-backward and update parameters to maximize likelihood."),
        ("What is a POS tagging task in NLP?",
        "The task of assigning parts of speech, such as noun or verb, to each word in a text."),
        ("How do RNNs handle sequential data?",
        "By maintaining a hidden state that captures dependencies across time steps."),
        ("What is the difference between feedforward neural networks and RNNs?",
        "Feedforward networks process fixed-sized inputs, while RNNs handle variable-length sequential data."),
        ("What is the purpose of backpropagation in neural networks?",
        "To compute gradients of the loss function and adjust weights to minimize error."),
        ("How do Long Short-Term Memory (LSTM) networks handle long-range dependencies?",
        "By using gates to control the flow of information, mitigating the vanishing gradient problem."),
        ("What is syntactic ambiguity in parsing?",
        "A situation where a sentence can have multiple valid parse trees."),
        ("What are the benefits of probabilistic context-free grammars (PCFGs)?",
        "They allow incorporating probabilities to resolve ambiguities in parse trees."),
        ("What is the role of a sequence labeling model in NLP?",
        "To predict a label for each element in a sequence, such as POS tagging or NER."),
        ("How does the log-sum-exp trick prevent underflow in NLP computations?",
        "By transforming summations into log space, avoiding numerical instability."),
        ("What is a prepositional phrase in syntax?",
        "A phrase starting with a preposition and ending with its object, functioning as an adverb or adjective."),
        ("What is the principle of compositionality in semantics?",
        "The meaning of a phrase depends on the meanings of its parts and how they are syntactically combined."),
        ("What is an idiom, and why does it violate compositionality?",
        "An idiom is an expression with a meaning not predictable from its parts, violating compositionality."),
        ("What is a hypernym-hyponym relationship?",
        "A hierarchical relationship where a hyponym is a more specific term under a hypernym, e.g., 'dog' under 'animal'."),
        ("What is the Sapir-Whorf hypothesis?",
        "It suggests language influences thought, with a strong version claiming language constrains all thought."),
        ("What is the noisy-channel model in machine translation?",
        "A statistical framework that selects the most probable translation given a source sentence using language and translation models."),
        ("What is the purpose of BLEU in machine translation?",
        "To evaluate translation quality by measuring n-gram overlap between the candidate and reference translations."),
        ("What are the two models in word2vec?",
        "Continuous Bag of Words (CBOW) and Skip-gram, used for learning word embeddings."),
        ("What is distributional semantics?",
        "An approach to understand word meaning based on the distribution of words in context."),
        ("What is pointwise mutual information (PMI)?",
        "A measure of word association strength, comparing observed co-occurrence to expected independent co-occurrence."),
        ("What is the role of lambda calculus in semantics?",
        "It formalizes the combination of meanings for parts of a sentence into a coherent whole."),
        ("What is the Vauquois triangle in machine translation?",
        "A framework illustrating different levels of translation, from direct word-for-word to deep semantic representations."),
        ("What is semantic inference?",
        "The process of making explicit information that is implicit in language."),
        ("What is Hearst's pattern for hypernym-hyponym extraction?",
        "Patterns like 'NP such as NP' and 'NP including NP' indicate hierarchical term relationships."),
        ("What is the significance of latent semantic analysis (LSA)?",
        "It reduces dimensionality in term-context matrices using SVD, capturing underlying semantics."),
        ("How does SVD help in distributional semantics?",
        "It compresses sparse term-context matrices into dense representations by minimizing reconstruction loss."),
        ("What is a definite description in semantics?",
        "A phrase that uniquely identifies an entity, such as 'the student who scored highest'."),
        ("What are universal and existential quantifiers in logic?",
        "Universal quantifiers (∀) assert properties for all entities; existential quantifiers (∃) assert properties for some entities."),
        ("What is lexical gap in machine translation?",
        "A lack of one-to-one correspondence for terms across languages, e.g., kinship terms in Chinese and English."),
        ("What is the challenge of idioms in translation?",
        "Idioms often cannot be directly translated as their meanings are not compositional."),
        ("What is co-compositionality?",
        "A phenomenon where the meaning of a word depends on the other words it combines with."),
        ("What is the primary challenge in multilingual NLP?", 
        "Limited labeled data and lack of support for low-resource languages."),
        ("What are pre-trained multilingual language models?", 
        "Models trained on multiple languages, such as mBERT, mT5, and XLM-R, designed to handle diverse linguistic data."),
        ("What is cross-lingual transfer learning?", 
        "Using a multilingual model to transfer knowledge from a high-resource language to a low-resource one."),
        ("What is language adaptation fine-tuning (LAFT)?", 
        "Fine-tuning a pre-trained multilingual model on monolingual corpora to adapt it to a specific language."),
        ("What is an example of a multilingual pre-trained model?", 
        "XLM-R, which supports over 100 languages."),
        ("What is the role of MasakhaNER?", 
        "A project for creating named entity recognition datasets for African languages."),
        ("What is summarization in NLP?", 
        "The process of creating a shorter version of a text while preserving its key information."),
        ("What is the difference between extractive and abstractive summarization?", 
        "Extractive summarization selects sentences directly from the text, while abstractive generates new sentences."),
        ("What is ROUGE in summarization evaluation?", 
        "A metric to compare the overlap of n-grams between a generated summary and a reference summary."),
        ("What is the importance of coreference resolution?", 
        "To identify expressions in a text that refer to the same entity for better text understanding."),
        ("What is the Hobbs algorithm used for?", 
        "A heuristic method for resolving pronouns and identifying antecedents in texts."),
        ("What are pleonastic pronouns?", 
        "Pronouns like 'it' that do not refer to specific entities, e.g., 'It is raining.'"),
        ("What is zero anaphora?", 
        "The omission of pronouns or subjects, common in languages like Japanese or Spanish."),
        ("What is anaphora in discourse?", 
        "A reference to an earlier expression in the text, such as 'he' referring to 'John.'"),
        ("What is cataphora?", 
        "A reference to an entity mentioned later in the text, e.g., 'When he arrived, John was happy.'"),
        ("What is event coreference resolution?", 
        "Determining whether two textual mentions refer to the same event."),
        ("What is the attention mechanism in neural machine translation?", 
        "It allows the model to focus on relevant parts of the source sentence while translating."),
        ("What are encoder-decoder architectures used for?", 
        "For tasks like machine translation, where the encoder processes input, and the decoder generates output."),
        ("What is the significance of transformers in NLP?", 
        "Transformers replaced RNNs for tasks like translation due to their parallel processing and better performance."),
        ("What are the key components of a summarization pipeline?", 
        "Content selection, transformation, and surface realization.")
    ]
    fine_tune_embeddings(train_data)

