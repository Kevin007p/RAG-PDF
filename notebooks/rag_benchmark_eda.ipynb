{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyzing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 COVIDQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'covidqa/train-00000-of-00001.parquet', 'test': 'covidqa/test-00000-of-00001.parquet', 'validation': 'covidqa/validation-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 CUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = {'train': 'cuad/train-00000-of-00001.parquet', 'validation': 'cuad/validation-00000-of-00001.parquet', 'test': 'cuad/test-00000-of-00001.parquet'}\n",
    "# df = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 FINQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = {'train': 'finqa/train-00000-of-00001.parquet', 'validation': 'finqa/validation-00000-of-00001.parquet', 'test': 'finqa/test-00000-of-00001.parquet'}\n",
    "# df = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print example question and calculate average length of questions\n",
    "print(f\"Example question: {df['question'][0]}\")\n",
    "average_question_length = df['question'].str.len().mean()\n",
    "print(f\"Average length of questions: {average_question_length}\")\n",
    "\n",
    "# Print example answer and calculate average length of answers\n",
    "print(f\"Example answer: {df['response'][0]}\")\n",
    "average_answer_length = df['response'].str.len().mean()\n",
    "print(f\"Average length of answers: {average_answer_length}\")\n",
    "\n",
    "# Print example documents and calculate average length of documents\n",
    "print(f\"Example documents: {df['documents'][0]}\")\n",
    "\n",
    "# Flatten the list of documents in the first row and calculate lengths\n",
    "document_lengths = [len(doc) for doc in df['documents'][0]]\n",
    "average_document_length = sum(document_lengths) / len(document_lengths)\n",
    "print(f\"Average length of documents: {average_document_length}\")\n",
    "\n",
    "# Calculate the average number of documents per row\n",
    "average_number_of_documents = df['documents'].apply(len).mean()\n",
    "print(f\"Average number of documents per row: {average_number_of_documents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Summarizing documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Loading Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarization model\n",
    "abstractive_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Generate abstractive summaries\n",
    "def generate_abstractive_summaries(documents):\n",
    "    return [abstractive_summarizer(doc, max_length=100, min_length=25, do_sample=False)[0]['summary_text'] for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractive\n",
    "from summarizer import Summarizer\n",
    "\n",
    "# Load the extractive summarization model\n",
    "extractive_summarizer = Summarizer()\n",
    "\n",
    "# Generate extractive summaries\n",
    "def generate_extractive_summaries(documents):\n",
    "    return [extractive_summarizer(doc, ratio=0.2) for doc in documents]  # 20% of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to the dataset\n",
    "df['abstractive_summary'] = df['documents'].apply(generate_abstractive_summaries)\n",
    "\n",
    "# Apply to the dataset\n",
    "df['extractive_summary'] = df['documents'].apply(generate_extractive_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Raw: {df['documents'][0]}\")\n",
    "print(f\"Extractive: {df['extractive_summary'][0]}\")\n",
    "print(f\"Abstractive: {df['abstractive_summary'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prepate data to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for fine-tuning\n",
    "def prepare_retrieval_training_data(df, input_column, target_column):\n",
    "    \"\"\"\n",
    "    Prepares training data for retrieval fine-tuning.\n",
    "    Each query is paired with its corresponding document (positive example).\n",
    "    \"\"\"\n",
    "    positive_pairs = [\n",
    "        InputExample(texts=[query, \" \".join(docs)], label=1.0) \n",
    "        for query, docs in zip(df['question'], df[input_column])\n",
    "    ]\n",
    "    return positive_pairs\n",
    "\n",
    "# Generate datasets for each setup\n",
    "raw_data = prepare_retrieval_training_data(df, \"documents\", \"response\")\n",
    "abstractive_data = prepare_retrieval_training_data(df, \"abstractive_summary\", \"response\")\n",
    "extractive_data = prepare_retrieval_training_data(df, \"extractive_summary\", \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model for retrieval\n",
    "def fine_tune_retriever(data, model_name, output_path):\n",
    "    \"\"\"\n",
    "    Fine-tune the retriever using MultipleNegativesRankingLoss.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    train_dataloader = DataLoader(data, shuffle=True, batch_size=16)\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "    \n",
    "    # Train\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=3,\n",
    "        warmup_steps=100,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Fine-tune for each dataset (could use multi-qa-mpnet-base-dot-v1)\n",
    "raw_retriever = fine_tune_retriever(raw_data, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_raw\")\n",
    "abstractive_retriever = fine_tune_retriever(abstractive_data, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_abstractive\")\n",
    "extractive_retriever = fine_tune_retriever(extractive_data, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_extractive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever_accuracy(retriever, df, k_values, input_column):\n",
    "    \"\"\"\n",
    "    Evaluates retrieval accuracy using Recall@k for different values of k.\n",
    "    \"\"\"\n",
    "    correct_retrievals = {k: 0 for k in k_values}\n",
    "    total_queries = len(df)\n",
    "\n",
    "    for query, relevant_docs in zip(df['question'], df[input_column]):\n",
    "        query_embedding = retriever.encode([query])\n",
    "        documents = [\" \".join(docs) for docs in df[input_column]]\n",
    "        document_embeddings = retriever.encode(documents)\n",
    "\n",
    "        distances = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
    "        distances.add(np.array(document_embeddings, dtype=\"float32\"))\n",
    "        _, indices = distances.search(np.array(query_embedding, dtype=\"float32\"), max(k_values))\n",
    "\n",
    "        for k in k_values:\n",
    "            retrieved_docs = [documents[idx] for idx in indices[0][:k]]\n",
    "            if any(doc in retrieved_docs for doc in relevant_docs):\n",
    "                correct_retrievals[k] += 1\n",
    "\n",
    "    recall_at_k = {k: correct_retrievals[k] / total_queries for k in k_values}\n",
    "    return recall_at_k\n",
    "\n",
    "# Evaluate each retriever\n",
    "k_values = [1, 5, 10]\n",
    "print(\"Raw Data Retriever Accuracy:\", evaluate_retriever_accuracy(raw_retriever, df, k_values, \"documents\"))\n",
    "print(\"Abstractive Retriever Accuracy:\", evaluate_retriever_accuracy(abstractive_retriever, df, k_values, \"abstractive_summary\"))\n",
    "print(\"Extractive Retriever Accuracy:\", evaluate_retriever_accuracy(extractive_retriever, df, k_values, \"extractive_summary\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load the model and tokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Prepare evaluation data\n",
    "def prepare_t5_inputs(df, input_column):\n",
    "    return [\n",
    "        {\"input_text\": f\"question: {query} context: {context}\", \"reference\": answer}\n",
    "        for query, context, answer in zip(df['question'], df[input_column], df['response'])\n",
    "    ]\n",
    "\n",
    "# Evaluate answer generation\n",
    "def evaluate_t5_model(data, t5_model, t5_tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluates the T5 model's answer quality using BLEU and ROUGE scores.\n",
    "    \"\"\"\n",
    "    bleu_metric = load_metric(\"bleu\")\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for sample in data:\n",
    "        inputs = t5_tokenizer(sample[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "        outputs = t5_model.generate(inputs['input_ids'], max_length=128, min_length=10)\n",
    "        predictions.append(t5_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        references.append(sample[\"reference\"])\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return bleu_score, rouge_score\n",
    "\n",
    "# Prepare data and evaluate each setup\n",
    "raw_t5_data = prepare_t5_inputs(df, \"documents\")\n",
    "abstractive_t5_data = prepare_t5_inputs(df, \"abstractive_summary\")\n",
    "extractive_t5_data = prepare_t5_inputs(df, \"extractive_summary\")\n",
    "\n",
    "print(\"Raw Data T5 Evaluation:\", evaluate_t5_model(raw_t5_data, t5_model, t5_tokenizer))\n",
    "print(\"Abstractive T5 Evaluation:\", evaluate_t5_model(abstractive_t5_data, t5_model, t5_tokenizer))\n",
    "print(\"Extractive T5 Evaluation:\", evaluate_t5_model(extractive_t5_data, t5_model, t5_tokenizer))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
