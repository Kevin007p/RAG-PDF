{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyzing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 COVIDQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'covidqa/train-00000-of-00001.parquet', 'test': 'covidqa/test-00000-of-00001.parquet', 'validation': 'covidqa/validation-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 CUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = {'train': 'cuad/train-00000-of-00001.parquet', 'validation': 'cuad/validation-00000-of-00001.parquet', 'test': 'cuad/test-00000-of-00001.parquet'}\n",
    "# df = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 FINQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = {'train': 'finqa/train-00000-of-00001.parquet', 'validation': 'finqa/validation-00000-of-00001.parquet', 'test': 'finqa/test-00000-of-00001.parquet'}\n",
    "# df = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                           question  \\\n",
      "0   358  What role does T-cell count play in severe hum...   \n",
      "1   457                                       What is MVO?   \n",
      "2  1172        What have sero-surveys of MERS virus found?   \n",
      "3   470       How were untreated MDA-MB-231 cells labeled?   \n",
      "4   518  What is the number of inhabitants of Reunion I...   \n",
      "\n",
      "                                           documents  \\\n",
      "0  [Title: Emergent severe acute respiratory dist...   \n",
      "1  [Title: Gene Knockdowns in Adult Animals: PPMO...   \n",
      "2  [Title: MERS coronavirus: diagnostics, epidemi...   \n",
      "3  [Title: Kinome-Wide siRNA Screening Identifies...   \n",
      "4  [Title: Pandemic Influenza Due to pH1N1/2009 V...   \n",
      "\n",
      "                                            response generation_model_name  \\\n",
      "0  The T-cell count plays a crucial role in sever...    gpt-3.5-turbo-1106   \n",
      "1  MVO is not defined or mentioned in the provide...    gpt-3.5-turbo-1106   \n",
      "2  Sero-surveys of MERS virus found no sign of ME...    gpt-3.5-turbo-1106   \n",
      "3  The untreated MDA-MB-231 cells were labeled wi...    gpt-3.5-turbo-1106   \n",
      "4  Reunion Island has a population of 850,000 inh...    gpt-3.5-turbo-1106   \n",
      "\n",
      "  annotating_model_name   dataset_name  \\\n",
      "0                gpt-4o  covidqa_train   \n",
      "1                gpt-4o  covidqa_train   \n",
      "2                gpt-4o  covidqa_train   \n",
      "3                gpt-4o  covidqa_train   \n",
      "4                gpt-4o  covidqa_train   \n",
      "\n",
      "                                 documents_sentences  \\\n",
      "0  [[[0a, Title: Emergent severe acute respirator...   \n",
      "1  [[[0a, Title: Gene Knockdowns in Adult Animals...   \n",
      "2  [[[0a, Title: MERS coronavirus: diagnostics, e...   \n",
      "3  [[[0a, Title: Kinome-Wide siRNA Screening Iden...   \n",
      "4  [[[0a, Title: Pandemic Influenza Due to pH1N1/...   \n",
      "\n",
      "                                  response_sentences  \\\n",
      "0  [[a, The T-cell count plays a crucial role in ...   \n",
      "1  [[a, MVO is not defined or mentioned in the pr...   \n",
      "2  [[a, Sero-surveys of MERS virus found no sign ...   \n",
      "3  [[a, The untreated MDA-MB-231 cells were label...   \n",
      "4  [[a, Reunion Island has a population of 850,00...   \n",
      "\n",
      "                        sentence_support_information  ...  \\\n",
      "0  [{'explanation': 'This sentence is supported b...  ...   \n",
      "1  [{'explanation': 'The sentence correctly state...  ...   \n",
      "2  [{'explanation': 'This sentence is partly supp...  ...   \n",
      "3  [{'explanation': 'The sentence is fully suppor...  ...   \n",
      "4  [{'explanation': 'This sentence claims that Re...  ...   \n",
      "\n",
      "  trulens_groundedness  trulens_context_relevance ragas_faithfulness  \\\n",
      "0                  NaN                        NaN                NaN   \n",
      "1                  NaN                        NaN                NaN   \n",
      "2                  NaN                        NaN                NaN   \n",
      "3                  NaN                        NaN                NaN   \n",
      "4                  NaN                        NaN                NaN   \n",
      "\n",
      "  ragas_context_relevance gpt3_adherence gpt3_context_relevance  \\\n",
      "0                     NaN            1.0               0.473684   \n",
      "1                     NaN            0.0               0.529412   \n",
      "2                     NaN            0.0               0.173913   \n",
      "3                     NaN            1.0               0.041667   \n",
      "4                     NaN            1.0               0.055556   \n",
      "\n",
      "   gpt35_utilization  relevance_score  utilization_score  completeness_score  \n",
      "0           0.368421         0.315789           0.315789                1.00  \n",
      "1           0.352941         0.588235           0.588235                1.00  \n",
      "2           0.086957         0.173913           0.130435                0.75  \n",
      "3           0.041667         0.083333           0.041667                0.50  \n",
      "4           0.055556         0.055556           0.055556                1.00  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example question: What role does T-cell count play in severe human adenovirus type 55 (HAdV-55) infection?\n",
      "Average length of questions: 58.62140575079872\n",
      "Example answer: The T-cell count plays a crucial role in severe human adenovirus type 55 (HAdV-55) infection, as patients with severe disease may have low T-cell counts. In the study mentioned, the only patient who recovered from severe infection had higher T-cell counts, suggesting that a lower T-cell count may be a risk factor for severe HAdV-55 infection.\n",
      "Average length of answers: 256.0159744408946\n",
      "Example documents: ['Title: Emergent severe acute respiratory distress syndrome caused by adenovirus type 55 in immunocompetent adults in 2013: a prospective observational study\\nPassage: Recent studies have shown that the immune system plays a crucial role in the clearance of HAdV viremia and survival of the host . Chen et al. reported that, in the acute phase of HAdV-55 infection, patients with severe disease may have high levels of dendritic cells and Th17 cells . In our study, the only patient who recovered from severe infection had higher T-cell counts. Three of the five patients had relatively low T-cell counts when admitted. Our results suggest that these three patients may have been relatively immunocompromised and that a lower T-cell count may be a risk'\n",
      " 'Title: Emergent severe acute respiratory distress syndrome caused by adenovirus type 55 in immunocompetent adults in 2013: a prospective observational study\\nPassage: Recent studies have shown that the immune system plays a crucial role in the clearance of HAdV viremia and survival of the host . Chen et al. reported that, in the acute phase of HAdV-55 infection, patients with severe disease may have high levels of dendritic cells and Th17 cells . In our study, the only patient who recovered from severe infection had higher T-cell counts. Three of the five patients had relatively low T-cell counts when admitted. Our results suggest that these three patients may have been relatively immunocompromised and that a lower T-cell count may be a risk'\n",
      " 'Title: Human adenovirus type 7 infection causes a more severe disease than type 3\\nPassage: Laboratory findings for the HAdV-7-positive inpatients were also significantly different from those infected by HAdV-3 . Specifically, the HAdV-7-positive inpatients had lower white blood cell count , platelet count . In contrast, hemoglobin and C-reactive protein levels, and the percentages of lymphocytes, neutrophils and positive sputum culture were found to be statistically similar.'\n",
      " 'Title: Human adenovirus type 7 infection causes a more severe disease than type 3\\nPassage: Imaging and laboratory data on admission and during hospitalization were collected. White blood cell count > 15,000/ μL was defined as leukocytosis, whereas that < 4000/μL was defined as leukopenia.']\n",
      "Average length of documents: 567.0\n",
      "Average number of documents per row: 4.0\n"
     ]
    }
   ],
   "source": [
    "# Print example question and calculate average length of questions\n",
    "print(f\"Example question: {df['question'][0]}\")\n",
    "average_question_length = df['question'].str.len().mean()\n",
    "print(f\"Average length of questions: {average_question_length}\")\n",
    "\n",
    "# Print example answer and calculate average length of answers\n",
    "print(f\"Example answer: {df['response'][0]}\")\n",
    "average_answer_length = df['response'].str.len().mean()\n",
    "print(f\"Average length of answers: {average_answer_length}\")\n",
    "\n",
    "# Print example documents and calculate average length of documents\n",
    "print(f\"Example documents: {df['documents'][0]}\")\n",
    "\n",
    "# Flatten the list of documents in the first row and calculate lengths\n",
    "document_lengths = [len(doc) for doc in df['documents'][0]]\n",
    "average_document_length = sum(document_lengths) / len(document_lengths)\n",
    "print(f\"Average length of documents: {average_document_length}\")\n",
    "\n",
    "# Calculate the average number of documents per row\n",
    "average_number_of_documents = df['documents'].apply(len).mean()\n",
    "print(f\"Average number of documents per row: {average_number_of_documents}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'question', 'documents', 'response', 'generation_model_name',\n",
      "       'annotating_model_name', 'dataset_name', 'documents_sentences',\n",
      "       'response_sentences', 'sentence_support_information',\n",
      "       'unsupported_response_sentence_keys', 'adherence_score',\n",
      "       'overall_supported_explanation', 'relevance_explanation',\n",
      "       'all_relevant_sentence_keys', 'all_utilized_sentence_keys',\n",
      "       'trulens_groundedness', 'trulens_context_relevance',\n",
      "       'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence',\n",
      "       'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score',\n",
      "       'utilization_score', 'completeness_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Summarizing documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Loading Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarization model\n",
    "abstractive_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Generate abstractive summaries\n",
    "def generate_abstractive_summaries(documents):\n",
    "    return [abstractive_summarizer(doc, max_length=100, min_length=25, do_sample=False)[0]['summary_text'] for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractive\n",
    "from summarizer import Summarizer\n",
    "\n",
    "# Load the extractive summarization model\n",
    "extractive_summarizer = Summarizer()\n",
    "\n",
    "# Generate extractive summaries\n",
    "def generate_extractive_summaries(documents):\n",
    "    return [extractive_summarizer(doc, ratio=0.2) for doc in documents]  # 20% of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to the dataset\n",
    "df['abstractive_summary'] = df['documents'].apply(generate_abstractive_summaries)\n",
    "\n",
    "# Apply to the dataset\n",
    "df['extractive_summary'] = df['documents'].apply(generate_extractive_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Raw: {df['documents'][0]}\")\n",
    "print(f\"Extractive: {df['extractive_summary'][0]}\")\n",
    "print(f\"Abstractive: {df['abstractive_summary'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prepate data to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for fine-tuning\n",
    "def prepare_retrieval_training_data(df, input_column, target_column):\n",
    "    \"\"\"\n",
    "    Prepares training data for retrieval fine-tuning.\n",
    "    Each query is paired with its corresponding document (positive example).\n",
    "    \"\"\"\n",
    "    positive_pairs = [\n",
    "        InputExample(texts=[query, \" \".join(docs)], label=1.0) \n",
    "        for query, docs in zip(df['question'], df[input_column])\n",
    "    ]\n",
    "    return positive_pairs\n",
    "\n",
    "# Generate datasets for each setup\n",
    "raw_data = prepare_retrieval_training_data(df, \"documents\", \"response\")\n",
    "abstractive_data = prepare_retrieval_training_data(df, \"abstractive_summary\", \"response\")\n",
    "extractive_data = prepare_retrieval_training_data(df, \"extractive_summary\", \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model for retrieval\n",
    "def fine_tune_retriever(data, model_name, output_path):\n",
    "    \"\"\"\n",
    "    Fine-tune the retriever using MultipleNegativesRankingLoss.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    train_dataloader = DataLoader(data, shuffle=True, batch_size=16)\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "    \n",
    "    # Train\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=3,\n",
    "        warmup_steps=100,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Fine-tune for each dataset (could use multi-qa-mpnet-base-dot-v1)\n",
    "raw_retriever = fine_tune_retriever(raw_data, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_raw\")\n",
    "abstractive_retriever = fine_tune_retriever(abstractive_data, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_abstractive\")\n",
    "extractive_retriever = fine_tune_retriever(extractive_data, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_extractive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever_accuracy(retriever, df, k_values, input_column):\n",
    "    \"\"\"\n",
    "    Evaluates retrieval accuracy using Recall@k for different values of k.\n",
    "    \"\"\"\n",
    "    correct_retrievals = {k: 0 for k in k_values}\n",
    "    total_queries = len(df)\n",
    "\n",
    "    for query, relevant_docs in zip(df['question'], df[input_column]):\n",
    "        query_embedding = retriever.encode([query])\n",
    "        documents = [\" \".join(docs) for docs in df[input_column]]\n",
    "        document_embeddings = retriever.encode(documents)\n",
    "\n",
    "        distances = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
    "        distances.add(np.array(document_embeddings, dtype=\"float32\"))\n",
    "        _, indices = distances.search(np.array(query_embedding, dtype=\"float32\"), max(k_values))\n",
    "\n",
    "        for k in k_values:\n",
    "            retrieved_docs = [documents[idx] for idx in indices[0][:k]]\n",
    "            if any(doc in retrieved_docs for doc in relevant_docs):\n",
    "                correct_retrievals[k] += 1\n",
    "\n",
    "    recall_at_k = {k: correct_retrievals[k] / total_queries for k in k_values}\n",
    "    return recall_at_k\n",
    "\n",
    "# Evaluate each retriever\n",
    "k_values = [1, 5, 10]\n",
    "print(\"Raw Data Retriever Accuracy:\", evaluate_retriever_accuracy(raw_retriever, df, k_values, \"documents\"))\n",
    "print(\"Abstractive Retriever Accuracy:\", evaluate_retriever_accuracy(abstractive_retriever, df, k_values, \"abstractive_summary\"))\n",
    "print(\"Extractive Retriever Accuracy:\", evaluate_retriever_accuracy(extractive_retriever, df, k_values, \"extractive_summary\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load the model and tokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Prepare evaluation data\n",
    "def prepare_t5_inputs(df, input_column):\n",
    "    return [\n",
    "        {\"input_text\": f\"question: {query} context: {context}\", \"reference\": answer}\n",
    "        for query, context, answer in zip(df['question'], df[input_column], df['response'])\n",
    "    ]\n",
    "\n",
    "# Evaluate answer generation\n",
    "def evaluate_t5_model(data, t5_model, t5_tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluates the T5 model's answer quality using BLEU and ROUGE scores.\n",
    "    \"\"\"\n",
    "    bleu_metric = load_metric(\"bleu\")\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for sample in data:\n",
    "        inputs = t5_tokenizer(sample[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "        outputs = t5_model.generate(inputs['input_ids'], max_length=128, min_length=10)\n",
    "        predictions.append(t5_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        references.append(sample[\"reference\"])\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return bleu_score, rouge_score\n",
    "\n",
    "# Prepare data and evaluate each setup\n",
    "raw_t5_data = prepare_t5_inputs(df, \"documents\")\n",
    "abstractive_t5_data = prepare_t5_inputs(df, \"abstractive_summary\")\n",
    "extractive_t5_data = prepare_t5_inputs(df, \"extractive_summary\")\n",
    "\n",
    "print(\"Raw Data T5 Evaluation:\", evaluate_t5_model(raw_t5_data, t5_model, t5_tokenizer))\n",
    "print(\"Abstractive T5 Evaluation:\", evaluate_t5_model(abstractive_t5_data, t5_model, t5_tokenizer))\n",
    "print(\"Extractive T5 Evaluation:\", evaluate_t5_model(extractive_t5_data, t5_model, t5_tokenizer))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
