{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyzing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 COVIDQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'covidqa/train-00000-of-00001.parquet', 'test': 'covidqa/test-00000-of-00001.parquet', 'validation': 'covidqa/validation-00000-of-00001.parquet'}\n",
    "df_covid = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"train\"])\n",
    "df_covid_test = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"test\"])\n",
    "df_covid_validation = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 CUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'cuad/train-00000-of-00001.parquet', 'validation': 'cuad/validation-00000-of-00001.parquet', 'test': 'cuad/test-00000-of-00001.parquet'}\n",
    "df_cuad = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"train\"])\n",
    "df_cuad_validation = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"validation\"])\n",
    "df_cuad_test = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 FINQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'finqa/train-00000-of-00001.parquet', 'validation': 'finqa/validation-00000-of-00001.parquet', 'test': 'finqa/test-00000-of-00001.parquet'}\n",
    "df_fin = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"train\"])\n",
    "df_fin_test = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"test\"])\n",
    "df_fin_validation = pd.read_parquet(\"hf://datasets/rungalileo/ragbench/\" + splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and save the head and shape of each dataframe\n",
    "with open('dataframes_info.txt', 'w') as f:\n",
    "    for name, dfs in [(\"COVIDQA\", [df_covid, df_covid_test, df_covid_validation]), \n",
    "                      (\"CUAD\", [df_cuad, df_cuad_test, df_cuad_validation]), \n",
    "                      (\"FINQA\", [df_fin, df_fin_test, df_fin_validation])]:\n",
    "        for i, df in enumerate(dfs):\n",
    "            set_name = [\"Train\", \"Test\", \"Validation\"][i]\n",
    "            head = df.head()\n",
    "            shape = df.shape\n",
    "            \n",
    "            print(f\"{name} {set_name} Head:\\n{head}\\n\")\n",
    "            print(f\"{name} {set_name} Shape: {shape}\\n\")\n",
    "            \n",
    "            f.write(f\"{name} {set_name} Head:\\n{head}\\n\\n\")\n",
    "            f.write(f\"{name} {set_name} Shape: {shape}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Summarizing documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Loading Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data if not already present\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# Initialize Abstractive Summarizer\n",
    "abstractive_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "extractive_summarizer = Summarizer()\n",
    "\n",
    "# Generate abstractive summaries\n",
    "def generate_abstractive_summaries(documents):\n",
    "    \"\"\"Generate abstractive summaries for a list of documents.\"\"\"\n",
    "    summaries = []\n",
    "    for doc in documents:\n",
    "        try:\n",
    "            summary = abstractive_summarizer(doc, max_length=100, min_length=25, do_sample=False)[0]['summary_text']\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in abstractive summarization: {e}\")\n",
    "            summaries.append(\"\")\n",
    "    return summaries\n",
    "\n",
    "# Generate extractive summaries\n",
    "def generate_extractive_summaries(documents):\n",
    "    return [extractive_summarizer(doc, ratio=0.6) for doc in documents]  # 60% of the text\n",
    "\n",
    "# Generate filtered summaries (word removal)\n",
    "def generate_filtered_summary(document):\n",
    "    \"\"\"Generate a filtered summary by removing stop words and non-alphabetic tokens.\"\"\"\n",
    "    try:\n",
    "        # Tokenize and convert to lowercase\n",
    "        tokens = word_tokenize(document.lower())\n",
    "        \n",
    "        # Define stop words\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        \n",
    "        # Filter tokens\n",
    "        filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "        \n",
    "        # Join tokens back into a string\n",
    "        return \" \".join(filtered_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in filtered summarization: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "fix this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the summarized dataframes\n",
    "df_covid_train_summarized = pd.read_parquet(\"covidqa_train_summarized.parquet\")\n",
    "df_covid_test_summarized = pd.read_parquet(\"covidqa_test_summarized.parquet\")\n",
    "df_covid_validation_summarized = pd.read_parquet(\"covidqa_validation_summarized.parquet\")\n",
    "\n",
    "df_cuad_train_summarized = pd.read_parquet(\"cuad_train_summarized.parquet\")\n",
    "df_cuad_test_summarized = pd.read_parquet(\"cuad_test_summarized.parquet\")\n",
    "df_cuad_validation_summarized = pd.read_parquet(\"cuad_validation_summarized.parquet\")\n",
    "\n",
    "df_fin_train_summarized = pd.read_parquet(\"finqa_train_summarized.parquet\")\n",
    "df_fin_test_summarized = pd.read_parquet(\"finqa_test_summarized.parquet\")\n",
    "df_fin_validation_summarized = pd.read_parquet(\"finqa_validation_summarized.parquet\")\n",
    "\n",
    "# Print samples to file\n",
    "with open('summarized_samples.txt', 'w') as f:\n",
    "    for name, dfs in [(\"COVIDQA\", [df_covid_train_summarized, df_covid_test_summarized, df_covid_validation_summarized]), \n",
    "                      (\"CUAD\", [df_cuad_train_summarized, df_cuad_test_summarized, df_cuad_validation_summarized]), \n",
    "                      (\"FINQA\", [df_fin_train_summarized, df_fin_test_summarized, df_fin_validation_summarized])]:\n",
    "        for i, df in enumerate(dfs):\n",
    "            set_name = [\"Train\", \"Test\", \"Validation\"][i]\n",
    "            sample = df.head(1)\n",
    "            \n",
    "            f.write(f\"{name} {set_name} Sample:\\n{sample}\\n\\n\")\n",
    "            f.write(f\"{name} {set_name} Raw Data:\\n{sample['raw_data'].values[0]}\\n\\n\")\n",
    "\n",
    "print(\"Samples saved to summarized_samples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def analyze_dataframe(df, name):\n",
    "    analysis = {}\n",
    "    \n",
    "    # Example question and average length of questions\n",
    "    analysis['example_question'] = df['question'][0]\n",
    "    analysis['average_question_length'] = df['question'].str.len().mean()\n",
    "    \n",
    "    # Example answer and average length of answers\n",
    "    analysis['example_answer'] = df['response'][0]\n",
    "    analysis['average_answer_length'] = df['response'].str.len().mean()\n",
    "    \n",
    "    # Example documents and average length of documents\n",
    "    analysis['example_documents'] = df['documents'][0]\n",
    "    document_lengths = [len(doc) for doc in df['documents'][0]]\n",
    "    analysis['average_document_length'] = sum(document_lengths) / len(document_lengths)\n",
    "    \n",
    "    # Average number of documents per row\n",
    "    analysis['average_number_of_documents'] = df['documents'].apply(len).mean()\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze each dataframe set\n",
    "datasets = {\n",
    "    'COVIDQA': [df_covid, df_covid_test, df_covid_validation],\n",
    "    'CUAD': [df_cuad, df_cuad_test, df_cuad_validation],\n",
    "    'FINQA': [df_fin, df_fin_test, df_fin_validation]\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, dfs in datasets.items():\n",
    "    results[name] = {}\n",
    "    for i, df in enumerate(dfs):\n",
    "        set_name = [\"Train\", \"Test\", \"Validation\"][i]\n",
    "        results[name][set_name] = analyze_dataframe(df, name)\n",
    "\n",
    "# Analyze summarized dataframes\n",
    "summarized_datasets = {\n",
    "    'COVIDQA': {\n",
    "        'Train': df_covid_train_summarized,\n",
    "        'Test': df_covid_test_summarized,\n",
    "        'Validation': df_covid_validation_summarized\n",
    "    },\n",
    "    'CUAD': {\n",
    "        'Train': df_cuad_train_summarized,\n",
    "        'Test': df_cuad_test_summarized,\n",
    "        'Validation': df_cuad_validation_summarized\n",
    "    },\n",
    "    'FINQA': {\n",
    "        'Train': df_fin_train_summarized,\n",
    "        'Test': df_fin_test_summarized,\n",
    "        'Validation': df_fin_validation_summarized\n",
    "    }\n",
    "}\n",
    "\n",
    "summarization_types = ['abstractive_summary', 'extractive_summary', 'filtered_summary']\n",
    "\n",
    "for name, dfs in summarized_datasets.items():\n",
    "    results[name + '_summarized'] = {}\n",
    "    for set_name, df in dfs.items():\n",
    "        results[name + '_summarized'][set_name] = {}\n",
    "        for summary_type in summarization_types:\n",
    "            results[name + '_summarized'][set_name][summary_type] = analyze_dataframe(df, name)\n",
    "\n",
    "# Save the results to a file\n",
    "with open('data_analysis_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Analysis results saved to data_analysis_results.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prepate data to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the summarized dataframes\n",
    "df_covid_train_summarized = pd.read_parquet(\"covidqa_train_summarized.parquet\")\n",
    "df_covid_test_summarized = pd.read_parquet(\"covidqa_test_summarized.parquet\")\n",
    "df_covid_validation_summarized = pd.read_parquet(\"covidqa_validation_summarized.parquet\")\n",
    "\n",
    "df_cuad_train_summarized = pd.read_parquet(\"cuad_train_summarized.parquet\")\n",
    "df_cuad_test_summarized = pd.read_parquet(\"cuad_test_summarized.parquet\")\n",
    "df_cuad_validation_summarized = pd.read_parquet(\"cuad_validation_summarized.parquet\")\n",
    "\n",
    "df_fin_train_summarized = pd.read_parquet(\"finqa_train_summarized.parquet\")\n",
    "df_fin_test_summarized = pd.read_parquet(\"finqa_test_summarized.parquet\")\n",
    "df_fin_validation_summarized = pd.read_parquet(\"finqa_validation_summarized.parquet\")\n",
    "\n",
    "# Combine all dataframes into a dictionary for easy access\n",
    "summarized_datasets = {\n",
    "    'COVIDQA': {\n",
    "        'Train': df_covid_train_summarized,\n",
    "        'Test': df_covid_test_summarized,\n",
    "        'Validation': df_covid_validation_summarized\n",
    "    },\n",
    "    'CUAD': {\n",
    "        'Train': df_cuad_train_summarized,\n",
    "        'Test': df_cuad_test_summarized,\n",
    "        'Validation': df_cuad_validation_summarized\n",
    "    },\n",
    "    'FINQA': {\n",
    "        'Train': df_fin_train_summarized,\n",
    "        'Test': df_fin_test_summarized,\n",
    "        'Validation': df_fin_validation_summarized\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"All summarized datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for fine-tuning\n",
    "def prepare_retrieval_training_data(df, input_column, target_column):\n",
    "    \"\"\"\n",
    "    Prepares training data for retrieval fine-tuning.\n",
    "    Each query is paired with its corresponding document (positive example).\n",
    "    \"\"\"\n",
    "    positive_pairs = [\n",
    "        InputExample(texts=[query, \" \".join(docs)], label=1.0) \n",
    "        for query, docs in zip(df['question'], df[input_column])\n",
    "    ]\n",
    "    return positive_pairs\n",
    "\n",
    "# Generate datasets for each setup\n",
    "raw_data_covidqa = prepare_retrieval_training_data(summarized_datasets['COVIDQA']['Train'], \"documents\", \"response\")\n",
    "abstractive_data_covidqa = prepare_retrieval_training_data(summarized_datasets['COVIDQA']['Train'], \"abstractive_summary\", \"response\")\n",
    "extractive_data_covidqa = prepare_retrieval_training_data(summarized_datasets['COVIDQA']['Train'], \"extractive_summary\", \"response\")\n",
    "filtered_data_covidqa = prepare_retrieval_training_data(summarized_datasets['COVIDQA']['Train'], \"filtered_summary\", \"response\")\n",
    "\n",
    "raw_data_cuad = prepare_retrieval_training_data(summarized_datasets['CUAD']['Train'], \"documents\", \"response\")\n",
    "abstractive_data_cuad = prepare_retrieval_training_data(summarized_datasets['CUAD']['Train'], \"abstractive_summary\", \"response\")\n",
    "extractive_data_cuad = prepare_retrieval_training_data(summarized_datasets['CUAD']['Train'], \"extractive_summary\", \"response\")\n",
    "filtered_data_cuad = prepare_retrieval_training_data(summarized_datasets['CUAD']['Train'], \"filtered_summary\", \"response\")\n",
    "\n",
    "raw_data_finqa = prepare_retrieval_training_data(summarized_datasets['FINQA']['Train'], \"documents\", \"response\")\n",
    "abstractive_data_finqa = prepare_retrieval_training_data(summarized_datasets['FINQA']['Train'], \"abstractive_summary\", \"response\")\n",
    "extractive_data_finqa = prepare_retrieval_training_data(summarized_datasets['FINQA']['Train'], \"extractive_summary\", \"response\")\n",
    "filtered_data_finqa = prepare_retrieval_training_data(summarized_datasets['FINQA']['Train'], \"filtered_summary\", \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model for retrieval\n",
    "def fine_tune_retriever(train_data, model_name, output_path):\n",
    "    \"\"\"\n",
    "    Fine-tune the retriever using MultipleNegativesRankingLoss.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "    \n",
    "    # Train\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=3,\n",
    "        warmup_steps=100,\n",
    "        evaluator=None,  # Add an evaluator if needed\n",
    "        evaluation_steps=1000,  # Evaluate every 1000 steps\n",
    "        output_path=output_path\n",
    "    )\n",
    "    model.save(output_path)\n",
    "    return model\n",
    "\n",
    "# Fine-tune for each dataset and summarization type\n",
    "raw_retriever_covidqa = fine_tune_retriever(raw_data_covidqa, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_raw_covidqa\")\n",
    "abstractive_retriever_covidqa = fine_tune_retriever(abstractive_data_covidqa, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_abstractive_covidqa\")\n",
    "extractive_retriever_covidqa = fine_tune_retriever(extractive_data_covidqa, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_extractive_covidqa\")\n",
    "filtered_retriever_covidqa = fine_tune_retriever(filtered_data_covidqa, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_filtered_covidqa\")\n",
    "\n",
    "raw_retriever_cuad = fine_tune_retriever(raw_data_cuad, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_raw_cuad\")\n",
    "abstractive_retriever_cuad = fine_tune_retriever(abstractive_data_cuad, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_abstractive_cuad\")\n",
    "extractive_retriever_cuad = fine_tune_retriever(extractive_data_cuad, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_extractive_cuad\")\n",
    "filtered_retriever_cuad = fine_tune_retriever(filtered_data_cuad, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_filtered_cuad\")\n",
    "\n",
    "raw_retriever_finqa = fine_tune_retriever(raw_data_finqa, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_raw_finqa\")\n",
    "abstractive_retriever_finqa = fine_tune_retriever(abstractive_data_finqa, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_abstractive_finqa\")\n",
    "extractive_retriever_finqa = fine_tune_retriever(extractive_data_finqa, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_extractive_finqa\")\n",
    "filtered_retriever_finqa = fine_tune_retriever(filtered_data_finqa, \"all-MiniLM-L6-v2\", \"fine_tuned_retriever_filtered_finqa\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever_accuracy(retriever, df, k_values, input_column):\n",
    "    \"\"\"\n",
    "    Evaluates retrieval accuracy using Recall@k for different values of k.\n",
    "    \"\"\"\n",
    "    correct_retrievals = {k: 0 for k in k_values}\n",
    "    total_queries = len(df)\n",
    "\n",
    "    for query, relevant_docs in zip(df['question'], df[input_column]):\n",
    "        query_embedding = retriever.encode([query])\n",
    "        documents = [\" \".join(docs) for docs in df[input_column]]\n",
    "        document_embeddings = retriever.encode(documents)\n",
    "\n",
    "        distances = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
    "        distances.add(np.array(document_embeddings, dtype=\"float32\"))\n",
    "        _, indices = distances.search(np.array(query_embedding, dtype=\"float32\"), max(k_values))\n",
    "\n",
    "        for k in k_values:\n",
    "            retrieved_docs = [documents[idx] for idx in indices[0][:k]]\n",
    "            if any(doc in retrieved_docs for doc in relevant_docs):\n",
    "                correct_retrievals[k] += 1\n",
    "\n",
    "    recall_at_k = {k: correct_retrievals[k] / total_queries for k in k_values}\n",
    "    return recall_at_k\n",
    "\n",
    "# Evaluate each retriever and print results to file\n",
    "k_values = [1, 4, 7]\n",
    "retrievers = {\n",
    "    \"Raw\": raw_retriever_covidqa,\n",
    "    \"Abstractive\": abstractive_retriever_covidqa,\n",
    "    \"Extractive\": extractive_retriever_covidqa,\n",
    "    \"Filtered\": filtered_retriever_covidqa\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"COVIDQA\": summarized_datasets['COVIDQA']['Train'],\n",
    "    \"CUAD\": summarized_datasets['CUAD']['Train'],\n",
    "    \"FINQA\": summarized_datasets['FINQA']['Train']\n",
    "}\n",
    "\n",
    "with open('retriever_accuracy_results.txt', 'w') as f:\n",
    "    for retriever_name, retriever in retrievers.items():\n",
    "        for dataset_name, df in datasets.items():\n",
    "            for summary_type in [\"documents\", \"abstractive_summary\", \"extractive_summary\", \"filtered_summary\"]:\n",
    "                accuracy = evaluate_retriever_accuracy(retriever, df, k_values, summary_type)\n",
    "                f.write(f\"{retriever_name} Retriever on {dataset_name} ({summary_type}):\\n\")\n",
    "                f.write(f\"Recall@k: {accuracy}\\n\\n\")\n",
    "\n",
    "print(\"Retriever accuracy results saved to retriever_accuracy_results.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from evaluate import load\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the model and tokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Prepare evaluation data\n",
    "def prepare_t5_inputs(df, input_column):\n",
    "    return [\n",
    "        {\"input_text\": f\"question: {query} context: {context}\", \"reference\": answer}\n",
    "        for query, context, answer in zip(df['question'], df[input_column], df['response'])\n",
    "    ]\n",
    "\n",
    "# Evaluate answer generation\n",
    "def evaluate_t5_model(data, t5_model, t5_tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluates the T5 model's answer quality using BLEU and ROUGE scores.\n",
    "    \"\"\"\n",
    "    bleu_metric = load(\"bleu\")\n",
    "    rouge_metric = load(\"rouge\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for sample in tqdm(data, desc=\"Evaluating T5 Model\"):\n",
    "        inputs = t5_tokenizer(\n",
    "            sample[\"input_text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "        outputs = t5_model.generate(inputs['input_ids'], max_length=128, min_length=10)\n",
    "        predictions.append(t5_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        references.append(sample[\"reference\"])\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return bleu_score, rouge_score\n",
    "\n",
    "# Function to evaluate and measure time and token usage\n",
    "def evaluate_and_measure(data, t5_model, t5_tokenizer, description):\n",
    "    start_time = time.time()\n",
    "    bleu_score, rouge_score = evaluate_t5_model(data, t5_model, t5_tokenizer)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    total_tokens = sum(len(t5_tokenizer(sample[\"input_text\"])['input_ids']) for sample in data)\n",
    "    return bleu_score, rouge_score, total_time, total_tokens\n",
    "\n",
    "# Prepare data and evaluate each setup\n",
    "raw_t5_data = prepare_t5_inputs(df, \"documents\")\n",
    "abstractive_t5_data = prepare_t5_inputs(df, \"abstractive_summary\")\n",
    "extractive_t5_data = prepare_t5_inputs(df, \"extractive_summary\")\n",
    "filtered_t5_data = prepare_t5_inputs(df, \"filtered_summary\")\n",
    "\n",
    "# Evaluate and measure for each setup\n",
    "results = {}\n",
    "for name, data in [(\"Raw\", raw_t5_data), (\"Abstractive\", abstractive_t5_data), (\"Extractive\", extractive_t5_data), (\"Filtered\", filtered_t5_data)]:\n",
    "    print(f\"Evaluating {name} data...\")\n",
    "    bleu, rouge, time_taken, tokens = evaluate_and_measure(data, t5_model, t5_tokenizer, name)\n",
    "    results[name] = {\n",
    "        \"BLEU\": bleu,\n",
    "        \"ROUGE\": rouge,\n",
    "        \"Time (s)\": time_taken,\n",
    "        \"Total Tokens\": tokens\n",
    "    }\n",
    "\n",
    "# Print results to file\n",
    "with open('t5_evaluation_results.txt', 'w') as f:\n",
    "    for name, metrics in results.items():\n",
    "        f.write(f\"{name} T5 Evaluation:\\n\")\n",
    "        f.write(f\"BLEU: {metrics['BLEU']}\\n\")\n",
    "        f.write(f\"ROUGE: {metrics['ROUGE']}\\n\")\n",
    "        f.write(f\"Time (s): {metrics['Time (s)']}\\n\")\n",
    "        f.write(f\"Total Tokens: {metrics['Total Tokens']}\\n\\n\")\n",
    "\n",
    "print(\"T5 evaluation results saved to t5_evaluation_results.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
